{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import gradcheck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"Un objet contexte très simplifié pour simuler PyTorch\n",
    "\n",
    "    Un contexte différent doit être utilisé à chaque forward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._saved_tensors = ()\n",
    "    def save_for_backward(self, *args):\n",
    "        self._saved_tensors = args\n",
    "    @property\n",
    "    def saved_tensors(self):\n",
    "        return self._saved_tensors\n",
    "\n",
    "\n",
    "class MSE(Function):\n",
    "    \"\"\"Début d'implementation de la fonction MSE\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhat, y):\n",
    "        ## Garde les valeurs nécessaires pour le backwards\n",
    "        ctx.save_for_backward(yhat, y)\n",
    "\n",
    "        return (y-yhat).pow(2).sum().mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ## Calcul du gradient du module par rapport a chaque groupe d'entrées\n",
    "        yhat, y = ctx.saved_tensors\n",
    "        #  TODO:  Renvoyer par les deux dérivées partielles (par rapport à yhat et à y)\n",
    "        return grad_output*-2*(y-yhat), grad_output*2*(y-yhat)\n",
    "\n",
    "#  TODO:  Implémenter la fonction Linear(X, W, b)sur le même modèle que MSE\n",
    "class Linear(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        return X@W+b\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X, W, b= ctx.saved_tensors\n",
    "        return grad_output@W.T, X.T@grad_output, grad_output\n",
    "\n",
    "    \n",
    "## Utile dans ce TP que pour le script tp1_gradcheck\n",
    "mse = MSE.apply\n",
    "linear = Linear.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randn(10,5, requires_grad=True, dtype=torch.float64)\n",
    "torch.autograd.gradcheck(mse, (yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.2917, -1.6823, -0.1485,  4.4168],\n",
       "        [-6.5817, -3.6313,  6.7623,  1.0391]], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 2, requires_grad=True, dtype = torch.float64)\n",
    "w = torch.randn(2, 4, requires_grad=True, dtype = torch.float64)\n",
    "b = torch.randn(4, requires_grad=True, dtype = torch.float64)\n",
    "\n",
    "# torch.autograd.gradcheck(linear, (x,w, b))\n",
    "y = torch.randn(3, 4)\n",
    "yhat = linear(x,w,b)\n",
    "assert yhat.shape == y.shape\n",
    "loss = mse(yhat, y)\n",
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations 0: loss 2585.802001953125\n",
      "Itérations 1: loss 2002.4383544921875\n",
      "Itérations 2: loss 1571.7857666015625\n",
      "Itérations 3: loss 1251.0384521484375\n",
      "Itérations 4: loss 1009.9608764648438\n",
      "Itérations 5: loss 827.0586547851562\n",
      "Itérations 6: loss 686.9537353515625\n",
      "Itérations 7: loss 578.5724487304688\n",
      "Itérations 8: loss 493.8895263671875\n",
      "Itérations 9: loss 427.0503845214844\n",
      "Itérations 10: loss 373.7559814453125\n",
      "Itérations 11: loss 330.8287048339844\n",
      "Itérations 12: loss 295.9032287597656\n",
      "Itérations 13: loss 267.2072448730469\n",
      "Itérations 14: loss 243.40325927734375\n",
      "Itérations 15: loss 223.4746551513672\n",
      "Itérations 16: loss 206.64321899414062\n",
      "Itérations 17: loss 192.3087615966797\n",
      "Itérations 18: loss 180.00485229492188\n",
      "Itérations 19: loss 169.36639404296875\n",
      "Itérations 20: loss 160.10531616210938\n",
      "Itérations 21: loss 151.99261474609375\n",
      "Itérations 22: loss 144.84490966796875\n",
      "Itérations 23: loss 138.51412963867188\n",
      "Itérations 24: loss 132.8798828125\n",
      "Itérations 25: loss 127.84352111816406\n",
      "Itérations 26: loss 123.32362365722656\n",
      "Itérations 27: loss 119.25251770019531\n",
      "Itérations 28: loss 115.57359313964844\n",
      "Itérations 29: loss 112.2391586303711\n",
      "Itérations 30: loss 109.208740234375\n",
      "Itérations 31: loss 106.44786071777344\n",
      "Itérations 32: loss 103.9269027709961\n",
      "Itérations 33: loss 101.6203384399414\n",
      "Itérations 34: loss 99.50601959228516\n",
      "Itérations 35: loss 97.56463623046875\n",
      "Itérations 36: loss 95.779296875\n",
      "Itérations 37: loss 94.1351318359375\n",
      "Itérations 38: loss 92.61900329589844\n",
      "Itérations 39: loss 91.21930694580078\n",
      "Itérations 40: loss 89.92565155029297\n",
      "Itérations 41: loss 88.72882843017578\n",
      "Itérations 42: loss 87.62055206298828\n",
      "Itérations 43: loss 86.59339141845703\n",
      "Itérations 44: loss 85.64067077636719\n",
      "Itérations 45: loss 84.75631713867188\n",
      "Itérations 46: loss 83.93490600585938\n",
      "Itérations 47: loss 83.17144775390625\n",
      "Itérations 48: loss 82.46144104003906\n",
      "Itérations 49: loss 81.80079650878906\n",
      "Itérations 50: loss 81.18577575683594\n",
      "Itérations 51: loss 80.61293029785156\n",
      "Itérations 52: loss 80.07917022705078\n",
      "Itérations 53: loss 79.58158874511719\n",
      "Itérations 54: loss 79.11756896972656\n",
      "Itérations 55: loss 78.6846923828125\n",
      "Itérations 56: loss 78.28073120117188\n",
      "Itérations 57: loss 77.90364074707031\n",
      "Itérations 58: loss 77.55152893066406\n",
      "Itérations 59: loss 77.22262573242188\n",
      "Itérations 60: loss 76.91534423828125\n",
      "Itérations 61: loss 76.62818908691406\n",
      "Itérations 62: loss 76.35977935791016\n",
      "Itérations 63: loss 76.10884094238281\n",
      "Itérations 64: loss 75.87417602539062\n",
      "Itérations 65: loss 75.65467834472656\n",
      "Itérations 66: loss 75.44935607910156\n",
      "Itérations 67: loss 75.25723266601562\n",
      "Itérations 68: loss 75.07745361328125\n",
      "Itérations 69: loss 74.9092025756836\n",
      "Itérations 70: loss 74.7516860961914\n",
      "Itérations 71: loss 74.6042251586914\n",
      "Itérations 72: loss 74.46614837646484\n",
      "Itérations 73: loss 74.33683776855469\n",
      "Itérations 74: loss 74.21574401855469\n",
      "Itérations 75: loss 74.1023178100586\n",
      "Itérations 76: loss 73.99605560302734\n",
      "Itérations 77: loss 73.89649200439453\n",
      "Itérations 78: loss 73.80322265625\n",
      "Itérations 79: loss 73.71581268310547\n",
      "Itérations 80: loss 73.63389587402344\n",
      "Itérations 81: loss 73.55712127685547\n",
      "Itérations 82: loss 73.48516082763672\n",
      "Itérations 83: loss 73.4177017211914\n",
      "Itérations 84: loss 73.35447692871094\n",
      "Itérations 85: loss 73.29519653320312\n",
      "Itérations 86: loss 73.23960876464844\n",
      "Itérations 87: loss 73.18749237060547\n",
      "Itérations 88: loss 73.13861846923828\n",
      "Itérations 89: loss 73.0927963256836\n",
      "Itérations 90: loss 73.04981231689453\n",
      "Itérations 91: loss 73.00950622558594\n",
      "Itérations 92: loss 72.9717025756836\n",
      "Itérations 93: loss 72.93624114990234\n",
      "Itérations 94: loss 72.90298461914062\n",
      "Itérations 95: loss 72.87177276611328\n",
      "Itérations 96: loss 72.84249877929688\n",
      "Itérations 97: loss 72.8150405883789\n",
      "Itérations 98: loss 72.78926849365234\n",
      "Itérations 99: loss 72.76509094238281\n",
      "Itérations 100: loss 72.74241638183594\n",
      "Itérations 101: loss 72.72113037109375\n",
      "Itérations 102: loss 72.701171875\n",
      "Itérations 103: loss 72.68241882324219\n",
      "Itérations 104: loss 72.66484069824219\n",
      "Itérations 105: loss 72.64832305908203\n",
      "Itérations 106: loss 72.63284301757812\n",
      "Itérations 107: loss 72.6183090209961\n",
      "Itérations 108: loss 72.60466003417969\n",
      "Itérations 109: loss 72.59185791015625\n",
      "Itérations 110: loss 72.57982635498047\n",
      "Itérations 111: loss 72.56855010986328\n",
      "Itérations 112: loss 72.5579605102539\n",
      "Itérations 113: loss 72.54801177978516\n",
      "Itérations 114: loss 72.53868103027344\n",
      "Itérations 115: loss 72.52992248535156\n",
      "Itérations 116: loss 72.52169799804688\n",
      "Itérations 117: loss 72.51397705078125\n",
      "Itérations 118: loss 72.50672149658203\n",
      "Itérations 119: loss 72.49991607666016\n",
      "Itérations 120: loss 72.4935302734375\n",
      "Itérations 121: loss 72.48753356933594\n",
      "Itérations 122: loss 72.48188781738281\n",
      "Itérations 123: loss 72.47660064697266\n",
      "Itérations 124: loss 72.47163391113281\n",
      "Itérations 125: loss 72.46696472167969\n",
      "Itérations 126: loss 72.46260070800781\n",
      "Itérations 127: loss 72.4584732055664\n",
      "Itérations 128: loss 72.45462036132812\n",
      "Itérations 129: loss 72.45098114013672\n",
      "Itérations 130: loss 72.44758605957031\n",
      "Itérations 131: loss 72.44438171386719\n",
      "Itérations 132: loss 72.44137573242188\n",
      "Itérations 133: loss 72.43855285644531\n",
      "Itérations 134: loss 72.43589782714844\n",
      "Itérations 135: loss 72.43341827392578\n",
      "Itérations 136: loss 72.43108367919922\n",
      "Itérations 137: loss 72.42887878417969\n",
      "Itérations 138: loss 72.42681884765625\n",
      "Itérations 139: loss 72.42488098144531\n",
      "Itérations 140: loss 72.42305755615234\n",
      "Itérations 141: loss 72.42134094238281\n",
      "Itérations 142: loss 72.41973876953125\n",
      "Itérations 143: loss 72.4182357788086\n",
      "Itérations 144: loss 72.41681671142578\n",
      "Itérations 145: loss 72.41548156738281\n",
      "Itérations 146: loss 72.41423034667969\n",
      "Itérations 147: loss 72.4130630493164\n",
      "Itérations 148: loss 72.41194915771484\n",
      "Itérations 149: loss 72.4109115600586\n",
      "Itérations 150: loss 72.40994262695312\n",
      "Itérations 151: loss 72.40902709960938\n",
      "Itérations 152: loss 72.40816497802734\n",
      "Itérations 153: loss 72.40736389160156\n",
      "Itérations 154: loss 72.40660095214844\n",
      "Itérations 155: loss 72.40589141845703\n",
      "Itérations 156: loss 72.40521240234375\n",
      "Itérations 157: loss 72.40458679199219\n",
      "Itérations 158: loss 72.40399169921875\n",
      "Itérations 159: loss 72.40342712402344\n",
      "Itérations 160: loss 72.40291595458984\n",
      "Itérations 161: loss 72.40242004394531\n",
      "Itérations 162: loss 72.40196228027344\n",
      "Itérations 163: loss 72.40152740478516\n",
      "Itérations 164: loss 72.40111541748047\n",
      "Itérations 165: loss 72.40074157714844\n",
      "Itérations 166: loss 72.40038299560547\n",
      "Itérations 167: loss 72.40003967285156\n",
      "Itérations 168: loss 72.39971923828125\n",
      "Itérations 169: loss 72.39942932128906\n",
      "Itérations 170: loss 72.39913177490234\n",
      "Itérations 171: loss 72.39888000488281\n",
      "Itérations 172: loss 72.39862823486328\n",
      "Itérations 173: loss 72.39839172363281\n",
      "Itérations 174: loss 72.39817810058594\n",
      "Itérations 175: loss 72.39796447753906\n",
      "Itérations 176: loss 72.39778137207031\n",
      "Itérations 177: loss 72.39759063720703\n",
      "Itérations 178: loss 72.39742279052734\n",
      "Itérations 179: loss 72.39726257324219\n",
      "Itérations 180: loss 72.39710998535156\n",
      "Itérations 181: loss 72.39696502685547\n",
      "Itérations 182: loss 72.3968276977539\n",
      "Itérations 183: loss 72.3967056274414\n",
      "Itérations 184: loss 72.39659118652344\n",
      "Itérations 185: loss 72.39646911621094\n",
      "Itérations 186: loss 72.39637756347656\n",
      "Itérations 187: loss 72.39627838134766\n",
      "Itérations 188: loss 72.39617919921875\n",
      "Itérations 189: loss 72.39610290527344\n",
      "Itérations 190: loss 72.39601135253906\n",
      "Itérations 191: loss 72.39594268798828\n",
      "Itérations 192: loss 72.39586639404297\n",
      "Itérations 193: loss 72.39580535888672\n",
      "Itérations 194: loss 72.39573669433594\n",
      "Itérations 195: loss 72.39567565917969\n",
      "Itérations 196: loss 72.39561462402344\n",
      "Itérations 197: loss 72.39556884765625\n",
      "Itérations 198: loss 72.39551544189453\n",
      "Itérations 199: loss 72.39546966552734\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tp1 import MSE, Linear, Context\n",
    "\n",
    "# Les données supervisées\n",
    "x = torch.randn(50, 13, requires_grad=True)\n",
    "y = torch.randn(50, 3)\n",
    "\n",
    "# Les paramètres du modèle à optimiser\n",
    "w = torch.randn(13, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "epsilon = 0.001\n",
    "\n",
    "writer = SummaryWriter()\n",
    "for n_iter in range(200):\n",
    "    ##  TODO:  Calcul du forward (loss)\n",
    "    yhat = linear(x,w,b)\n",
    "    loss = mse(yhat, y)\n",
    "    # `loss` doit correspondre au coût MSE calculé à cette itération\n",
    "    # on peut visualiser avec\n",
    "    # tensorboard --logdir runs/\n",
    "    writer.add_scalar('Loss/train', loss, n_iter)\n",
    "\n",
    "    # Sortie directe\n",
    "    print(f\"Itérations {n_iter}: loss {loss}\")\n",
    "\n",
    "    # Calcul du backward (grad_w, grad_b)\n",
    "    loss.backward()\n",
    "    # Mise à jour des paramètres du modèle\n",
    "    with torch.no_grad():\n",
    "        w -= epsilon*w.grad\n",
    "        b -= epsilon*b.grad\n",
    "        w.grad = None\n",
    "        b.grad = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
