{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6d475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading datasets...\n",
      "INFO:root:Vocabulary size: 42926\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datamaestro import prepare_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from typing import List\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from pathlib import Path\n",
    "\n",
    "ds = prepare_dataset('org.universaldependencies.french.gsd')\n",
    "\n",
    "\n",
    "# Format de sortie décrit dans\n",
    "# https://pypi.org/project/conllu/\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement s'il n'est pas connu\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "    OOVID = 1\n",
    "    PAD = 0\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        \"\"\" oov : autorise ou non les mots OOV \"\"\"\n",
    "        self.oov =  oov\n",
    "        self.id2word = [ \"PAD\"]\n",
    "        self.word2id = { \"PAD\" : Vocabulary.PAD}\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self,idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self,idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "\n",
    "class TaggingDataset():\n",
    "    def __init__(self, data, words: Vocabulary, tags: Vocabulary, adding=True):\n",
    "        self.sentences = []\n",
    "\n",
    "        for s in data:\n",
    "            self.sentences.append(([words.get(token[\"form\"], adding) for token in s], [tags.get(token[\"upostag\"], adding) for token in s]))\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate using pad_sequence\"\"\"\n",
    "    return tuple(pad_sequence([torch.LongTensor(b[j]) for b in batch]) for j in range(2))\n",
    "\n",
    "def collate_fn_ukn(batch, id_ukn=None, replace_ukn_rate = 0.0):\n",
    "    \"\"\"Collate using pad_sequence with ukn token replacement\"\"\"\n",
    "    X, Y = [], []\n",
    "    for b in batch:\n",
    "        # replace tokens with id_unknown with Bernoulli distribution\n",
    "        data_x = torch.LongTensor(b[0])\n",
    "        p = replace_ukn_rate\n",
    "        mask = torch.distributions.Bernoulli(probs=(1-p)).sample(data_x.size())\n",
    "        data_x[~mask.bool()] = id_ukn\n",
    "        # concatenate the data\n",
    "        X.append(data_x)\n",
    "        Y.append(torch.LongTensor(b[1]))\n",
    "    return (pad_sequence(X), pad_sequence(Y))\n",
    "\n",
    "logging.info(\"Loading datasets...\")\n",
    "words = Vocabulary(True)\n",
    "tags = Vocabulary(False)\n",
    "train_data = TaggingDataset(ds.train, words, tags, True)\n",
    "dev_data = TaggingDataset(ds.validation, words, tags, True)\n",
    "test_data = TaggingDataset(ds.test, words, tags, False)\n",
    "\n",
    "\n",
    "logging.info(\"Vocabulary size: %d\", len(words))\n",
    "\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "train_loader = DataLoader(train_data, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Only in train and dev \n",
    "ID_OOV = words.word2id['__OOV__']\n",
    "P_OOV = 0.01\n",
    "train_loader_ukn = DataLoader(train_data, collate_fn=lambda x : collate_fn_ukn(x, ID_OOV, P_OOV),\\\n",
    "                              batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_ukn = DataLoader(dev_data, collate_fn=lambda x : collate_fn_ukn(x, ID_OOV, P_OOV),\\\n",
    "                            batch_size=BATCH_SIZE)\n",
    "\n",
    "#  TODO:  Implémenter le modèle et la boucle d'apprentissage (en utilisant les LSTMs de pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd76ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_tagging(nn.Module):\n",
    "    def __init__(self, vocab_size, in_size, vocab_out_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=in_size)\n",
    "        # encoder\n",
    "        self.encoder = nn.LSTM(input_size=in_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x: tensor 3-d (Len, Batch)\n",
    "        \"\"\"\n",
    "        # embedding\n",
    "        x = self.embedding(x)\n",
    "        # encode\n",
    "        out, (h_n, c_n) = self.encoder(x)\n",
    "        out_final = self.decoder(out)\n",
    "        return out_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c78061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    loss_test = 0.\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = tagger(x)\n",
    "        loss = loss_fn(y_hat.permute(0, 2, 1), y)\n",
    "        loss_test += loss.item()\n",
    "    model.train()\n",
    "    return loss_test/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b28776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, model, optim):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.epoch = 0\n",
    "        self.iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b3d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title seq2seq pour le tagging with ukn\n",
    "savepath = Path(\"TAGGER.plt\")\n",
    "\n",
    "EPOCHS = 20\n",
    "hidden_size = 200\n",
    "VOCAB_SIZE_WORD = len(words)\n",
    "VOCAB_SIZE_TAG = len(tags)\n",
    "num_layers = 2\n",
    "VECT_EMB_SIZE = 150\n",
    "LR = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if savepath.is_file():\n",
    "    with savepath.open('rb') as fp:\n",
    "        state = torch.load(fp)\n",
    "else:\n",
    "    # RNN tagger\n",
    "    tagger = model_tagging(vocab_size=VOCAB_SIZE_WORD,\\\n",
    "                                   in_size=VECT_EMB_SIZE,\\\n",
    "                                   vocab_out_size=VOCAB_SIZE_TAG,\\\n",
    "                                   hidden_size=hidden_size,\\\n",
    "                                   num_layers=num_layers)\n",
    "    # optimizer\n",
    "    tagger.to(device)\n",
    "    optim = torch.optim.Adam(tagger.parameters(), lr=LR)\n",
    "    state = State(model = tagger, optim = optim)\n",
    "    \n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tags.word2id['PAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24db4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 13:01:01.986175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 13:01:04.053898: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 13:01:08.718732: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /Infos/lmd/2021/licence/ue/LU3IN029-2021oct/kO6/bin/gcc/lib64::/usr/local/cuda-11.6/lib64:/usr/local/cuda-12.2/lib64:/opt/gurobi801/linux64/lib\n",
      "2023-11-13 13:01:08.721637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /Infos/lmd/2021/licence/ue/LU3IN029-2021oct/kO6/bin/gcc/lib64::/usr/local/cuda-11.6/lib64:/usr/local/cuda-12.2/lib64:/opt/gurobi801/linux64/lib\n",
      "2023-11-13 13:01:08.721681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|████████████████████████████████████████████████████████████████████| 20/20 [01:34<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# use loader with UKN (cell 1)\n",
    "state.model.train()\n",
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(state.epoch, EPOCHS)):\n",
    "    for x, y in train_loader_ukn:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = state.model(x)\n",
    "        state.optim.zero_grad()\n",
    "        loss = loss_fn(y_hat.permute(0, 2, 1), y)\n",
    "        loss.backward()\n",
    "        state.optim.step()\n",
    "        state.iteration += 1\n",
    "        writer.add_scalar(\"Loss/train\", loss, state.iteration)\n",
    "\n",
    "    with savepath.open('wb') as fp:\n",
    "        state.epoch = epoch + 1\n",
    "        torch.save(state, fp)\n",
    "        \n",
    "    loss_eval = eval_model(test_loader, state.model, loss_fn, device)\n",
    "    writer.add_scalar(\"Loss/test\", loss_eval, state.epoch)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6bc5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\n",
      "['Louis', 'Bastien', '(', 'né', 'le', '26', 'octobre', '1881', 'à', 'Paris', 'et', 'mort', 'le', '13', 'août', '1963', 'à', '__OOV__', ')', 'est', 'un', 'coureur', 'cycliste', 'et', 'escrimeur', 'français', 'du', 'de', 'le', 'début', 'du', 'de', 'le', 'XXe', 'siècle', ',', 'dont', 'la', 'spécialité', 'était', 'le', 'cyclisme', 'sur', 'piste', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction\n",
      "['PROPN', 'PROPN', 'PUNCT', 'VERB', 'DET', 'NUM', 'NOUN', 'NUM', 'ADP', 'PROPN', 'CCONJ', 'VERB', 'DET', 'NUM', 'NOUN', 'NUM', 'ADP', 'PROPN', 'PUNCT', 'AUX', 'DET', 'NOUN', 'ADJ', 'CCONJ', 'VERB', 'ADJ', '_', 'ADP', 'DET', 'NOUN', '_', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'DET', 'NOUN', 'AUX', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PROPN', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']\n",
      "Ground truth\n",
      "['PROPN', 'PROPN', 'PUNCT', 'VERB', 'DET', 'NUM', 'NOUN', 'NUM', 'ADP', 'PROPN', 'CCONJ', 'VERB', 'DET', 'NUM', 'NOUN', 'NUM', 'ADP', 'PROPN', 'PUNCT', 'AUX', 'DET', 'NOUN', 'ADJ', 'CCONJ', 'NOUN', 'ADJ', '_', 'ADP', 'DET', 'NOUN', '_', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'DET', 'NOUN', 'AUX', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Sentence\n",
      "['__OOV__', ',', 'un', 'film', 'sur', 'la', 'vie', 'de', 'Hughes', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction\n",
      "['PRON', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'VERB', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']\n",
      "Ground truth\n",
      "['PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Sentence\n",
      "['Travail', 'de', 'très', 'grande', 'qualité', 'exécuté', 'par', 'un', 'imprimeur', '__OOV__', 'passionné', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction\n",
      "['PROPN', 'ADP', 'ADV', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'VERB', 'PUNCT', 'VERB', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']\n",
      "Ground truth\n",
      "['NOUN', 'ADP', 'ADV', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADJ', 'PUNCT', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Sentence\n",
      "['__OOV__', '__OOV__', '(', 'secrétaire', 'général', 'responsable', 'de', \"l'\", 'organisation', ')', 'et', 'Steve', '__OOV__', '(', 'secrétaire', 'général', 'responsable', 'des', 'de', 'les', 'projets', 'politiques', ',', 'du', 'de', 'le', 'renouveau', 'intellectuel', ',', 'de', 'la', 'formation', 'et', 'des', 'de', 'les', 'relations', 'internationales', ')', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction\n",
      "['PRON', 'VERB', 'PUNCT', 'NOUN', 'ADJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'PROPN', 'PUNCT', 'PUNCT', 'NOUN', 'ADJ', 'ADJ', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', '_', 'ADP', 'DET', 'NOUN', 'PROPN', 'PUNCT', 'ADP', 'DET', 'NOUN', 'CCONJ', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PUNCT', 'VERB', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']\n",
      "Ground truth\n",
      "['PROPN', 'PROPN', 'PUNCT', 'NOUN', 'ADJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'PROPN', 'PROPN', 'PUNCT', 'NOUN', 'ADJ', 'ADJ', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'ADP', 'DET', 'NOUN', 'CCONJ', '_', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PUNCT', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Sentence\n",
      "['Comprenant', 'six', 'sommets', 'dont', 'un', 'point', 'culminant', 'à', '__OOV__', 'mètres', 'et', 'une', 'arrivée', 'en', 'altitude', ',', \"c'\", 'est', 'une', 'étape', 'typique', 'de', 'montagne', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction\n",
      "['VERB', 'NUM', 'NOUN', 'PRON', 'PRON', 'NOUN', 'ADJ', 'ADP', 'VERB', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'DET', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'PUNCT', 'VERB', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET', 'DET']\n",
      "Ground truth\n",
      "['VERB', 'NUM', 'NOUN', 'PRON', 'DET', 'NOUN', 'VERB', 'ADP', 'NUM', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'DET', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'PUNCT', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    state.model.eval()\n",
    "    nb_visualisation = 5\n",
    "    x, y = next(iter(test_loader))\n",
    "    # predict\n",
    "    x= x.to(device)\n",
    "    yhat = state.model(x)\n",
    "    pred = torch.argmax(torch.softmax(yhat, dim=-1), dim=-1)\n",
    "    # visualiser\n",
    "    x, pred, y = x.permute(1, 0), pred.permute(1, 0), y.permute(1, 0) # batch, len\n",
    "    idx_random = torch.randint(len(x),(nb_visualisation,))\n",
    "    x_rand, pred_rand, y_rand = x[idx_random], pred[idx_random], y[idx_random]\n",
    "    for i in range(len(x_rand)):\n",
    "        print(\"Sentence\")\n",
    "        print(words.getwords(x_rand[i]))\n",
    "        print(\"Prediction\")\n",
    "        print(tags.getwords(pred_rand[i]))\n",
    "        print(\"Ground truth\")\n",
    "        print(tags.getwords(y_rand[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fa1f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-22c2459a041e8f8e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-22c2459a041e8f8e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff2487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
