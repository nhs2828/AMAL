{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('housing.csv', header=None, delimiter='\\s+', names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       float64\n",
       "ZN         float64\n",
       "INDUS      float64\n",
       "CHAS         int64\n",
       "NOX        float64\n",
       "RM         float64\n",
       "AGE        float64\n",
       "DIS        float64\n",
       "RAD          int64\n",
       "TAX        float64\n",
       "PTRATIO    float64\n",
       "B          float64\n",
       "LSTAT      float64\n",
       "MEDV       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df=(df-df.min())/(df.max()-df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_df.drop(columns=['MEDV']).values\n",
    "Y = normalized_df['MEDV'].values\n",
    "X = torch.from_numpy(X)\n",
    "Y = torch.from_numpy(Y)\n",
    "X = X.type(torch.float32)\n",
    "Y = Y.type(torch.float32).unsqueeze(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size= 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = X_train.shape[1]\n",
    "out_dim = Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(yhat, y):\n",
    "    return (y-yhat).pow(2).sum()\n",
    "\n",
    "def forward(X, W, B):\n",
    "    return torch.mm(X, W.T) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations 0: loss-train 70.35777282714844   lostt-test 15.377771377563477\n",
      "Itérations 1: loss-train 51.730594635009766   lostt-test 11.235861778259277\n",
      "Itérations 2: loss-train 41.24653244018555   lostt-test 8.663492202758789\n",
      "Itérations 3: loss-train 35.75853729248047   lostt-test 7.327631950378418\n",
      "Itérations 4: loss-train 31.385557174682617   lostt-test 6.372774124145508\n",
      "Itérations 5: loss-train 28.123674392700195   lostt-test 5.89354133605957\n",
      "Itérations 6: loss-train 24.872304916381836   lostt-test 5.027865409851074\n",
      "Itérations 7: loss-train 23.543476104736328   lostt-test 4.8279032707214355\n",
      "Itérations 8: loss-train 21.722986221313477   lostt-test 4.3436760902404785\n",
      "Itérations 9: loss-train 20.509841918945312   lostt-test 4.144069671630859\n",
      "Itérations 10: loss-train 20.051599502563477   lostt-test 4.191751956939697\n",
      "Itérations 11: loss-train 18.595417022705078   lostt-test 3.730201005935669\n",
      "Itérations 12: loss-train 17.74350357055664   lostt-test 3.542036533355713\n",
      "Itérations 13: loss-train 17.053691864013672   lostt-test 3.368311882019043\n",
      "Itérations 14: loss-train 16.424312591552734   lostt-test 3.2533555030822754\n",
      "Itérations 15: loss-train 15.704838752746582   lostt-test 3.1306936740875244\n",
      "Itérations 16: loss-train 15.158570289611816   lostt-test 3.002692461013794\n",
      "Itérations 17: loss-train 14.839703559875488   lostt-test 2.9197230339050293\n",
      "Itérations 18: loss-train 14.339814186096191   lostt-test 2.8278772830963135\n",
      "Itérations 19: loss-train 14.106501579284668   lostt-test 2.914304733276367\n",
      "Itérations 20: loss-train 13.5199556350708   lostt-test 2.667712688446045\n",
      "Itérations 21: loss-train 13.082387924194336   lostt-test 2.5985007286071777\n",
      "Itérations 22: loss-train 12.805039405822754   lostt-test 2.5381813049316406\n",
      "Itérations 23: loss-train 12.540565490722656   lostt-test 2.5303614139556885\n",
      "Itérations 24: loss-train 12.24923324584961   lostt-test 2.449279546737671\n",
      "Itérations 25: loss-train 11.9549560546875   lostt-test 2.3934013843536377\n",
      "Itérations 26: loss-train 11.844172477722168   lostt-test 2.427360773086548\n",
      "Itérations 27: loss-train 11.528336524963379   lostt-test 2.3153603076934814\n",
      "Itérations 28: loss-train 11.326109886169434   lostt-test 2.306082248687744\n",
      "Itérations 29: loss-train 11.058318138122559   lostt-test 2.2280995845794678\n",
      "Itérations 30: loss-train 10.895206451416016   lostt-test 2.1969916820526123\n",
      "Itérations 31: loss-train 10.78243350982666   lostt-test 2.1838529109954834\n",
      "Itérations 32: loss-train 10.620589256286621   lostt-test 2.1336514949798584\n",
      "Itérations 33: loss-train 10.4317626953125   lostt-test 2.101961851119995\n",
      "Itérations 34: loss-train 10.2781400680542   lostt-test 2.066361427307129\n",
      "Itérations 35: loss-train 10.164048194885254   lostt-test 2.057417631149292\n",
      "Itérations 36: loss-train 10.070096015930176   lostt-test 2.039027214050293\n",
      "Itérations 37: loss-train 9.983607292175293   lostt-test 1.996362328529358\n",
      "Itérations 38: loss-train 9.830514907836914   lostt-test 1.9879134893417358\n",
      "Itérations 39: loss-train 9.69001293182373   lostt-test 1.9618995189666748\n",
      "Itérations 40: loss-train 9.735921859741211   lostt-test 1.951087474822998\n",
      "Itérations 41: loss-train 9.482670783996582   lostt-test 1.9028215408325195\n",
      "Itérations 42: loss-train 9.325843811035156   lostt-test 1.8866190910339355\n",
      "Itérations 43: loss-train 9.288912773132324   lostt-test 1.88295578956604\n",
      "Itérations 44: loss-train 9.145917892456055   lostt-test 1.8371646404266357\n",
      "Itérations 45: loss-train 9.081509590148926   lostt-test 1.822563648223877\n",
      "Itérations 46: loss-train 9.252906799316406   lostt-test 1.901768445968628\n",
      "Itérations 47: loss-train 8.943109512329102   lostt-test 1.8156609535217285\n",
      "Itérations 48: loss-train 8.834110260009766   lostt-test 1.7910163402557373\n",
      "Itérations 49: loss-train 8.787274360656738   lostt-test 1.7808219194412231\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand((out_dim, nb_features), requires_grad=True)\n",
    "B = torch.rand(out_dim, requires_grad=True)\n",
    "NB_EPOCHS = 50\n",
    "epsilon = 1e-3\n",
    "writter = SummaryWriter()\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    for _ in range(len(X_train)):\n",
    "        random_inx = np.random.randint(0, len(X_train))\n",
    "        x_sgd = X_train[random_inx : random_inx+1]\n",
    "        y_sgd = Y_train[random_inx : random_inx+1]\n",
    "        y_hat_sgd = forward(x_sgd, W, B)\n",
    "        loss_sgd = mse(y_hat_sgd, y_sgd)\n",
    "        loss_sgd.backward()\n",
    "        with torch.no_grad():\n",
    "            W -= epsilon*W.grad\n",
    "            B -= epsilon*B.grad\n",
    "            W.grad = None\n",
    "            B.grad = None\n",
    "\n",
    "    loss_train = mse(forward(X_train, W, B), Y_train)\n",
    "    loss_val = mse(forward(X_val, W, B), Y_val)\n",
    "\n",
    "    writter.add_scalar('Loss/train', loss_train, epoch)\n",
    "    writter.add_scalar('Loss/val', loss_val, epoch)\n",
    "    # Sortie directe\n",
    "    print(f\"Itérations {epoch}: loss-train {loss_train}   lostt-test {loss_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations 0: loss-train 122.96549224853516   lostt-test 29.495174407958984\n",
      "Itérations 1: loss-train 87.59003448486328   lostt-test 20.966278076171875\n",
      "Itérations 2: loss-train 65.94181060791016   lostt-test 15.746397018432617\n",
      "Itérations 3: loss-train 52.47607421875   lostt-test 12.504424095153809\n",
      "Itérations 4: loss-train 43.91236877441406   lostt-test 10.450152397155762\n",
      "Itérations 5: loss-train 38.30202102661133   lostt-test 9.113073348999023\n",
      "Itérations 6: loss-train 34.48576736450195   lostt-test 8.212508201599121\n",
      "Itérations 7: loss-train 31.772357940673828   lostt-test 7.580570697784424\n",
      "Itérations 8: loss-train 29.748218536376953   lostt-test 7.116462707519531\n",
      "Itérations 9: loss-train 28.164670944213867   lostt-test 6.759342670440674\n",
      "Itérations 10: loss-train 26.871082305908203   lostt-test 6.472214698791504\n",
      "Itérations 11: loss-train 25.775272369384766   lostt-test 6.232352256774902\n",
      "Itérations 12: loss-train 24.819976806640625   lostt-test 6.025599956512451\n",
      "Itérations 13: loss-train 23.96895980834961   lostt-test 5.84298038482666\n",
      "Itérations 14: loss-train 23.198667526245117   lostt-test 5.678670406341553\n",
      "Itérations 15: loss-train 22.49332046508789   lostt-test 5.528777599334717\n",
      "Itérations 16: loss-train 21.84193992614746   lostt-test 5.39061975479126\n",
      "Itérations 17: loss-train 21.236595153808594   lostt-test 5.262279033660889\n",
      "Itérations 18: loss-train 20.671306610107422   lostt-test 5.142335414886475\n",
      "Itérations 19: loss-train 20.14140510559082   lostt-test 5.029695987701416\n",
      "Itérations 20: loss-train 19.643123626708984   lostt-test 4.923497676849365\n",
      "Itérations 21: loss-train 19.1733341217041   lostt-test 4.823038578033447\n",
      "Itérations 22: loss-train 18.72939682006836   lostt-test 4.727734088897705\n",
      "Itérations 23: loss-train 18.309024810791016   lostt-test 4.637092113494873\n",
      "Itérations 24: loss-train 17.910240173339844   lostt-test 4.550686836242676\n",
      "Itérations 25: loss-train 17.531288146972656   lostt-test 4.46815299987793\n",
      "Itérations 26: loss-train 17.170625686645508   lostt-test 4.389170169830322\n",
      "Itérations 27: loss-train 16.82687759399414   lostt-test 4.31345796585083\n",
      "Itérations 28: loss-train 16.49880027770996   lostt-test 4.2407660484313965\n",
      "Itérations 29: loss-train 16.18528938293457   lostt-test 4.17087459564209\n",
      "Itérations 30: loss-train 15.885343551635742   lostt-test 4.103586196899414\n",
      "Itérations 31: loss-train 15.598053932189941   lostt-test 4.03872537612915\n",
      "Itérations 32: loss-train 15.322603225708008   lostt-test 3.9761335849761963\n",
      "Itérations 33: loss-train 15.058242797851562   lostt-test 3.9156687259674072\n",
      "Itérations 34: loss-train 14.80429458618164   lostt-test 3.8572025299072266\n",
      "Itérations 35: loss-train 14.560133934020996   lostt-test 3.80061936378479\n",
      "Itérations 36: loss-train 14.32519245147705   lostt-test 3.74581241607666\n",
      "Itérations 37: loss-train 14.098943710327148   lostt-test 3.6926867961883545\n",
      "Itérations 38: loss-train 13.880912780761719   lostt-test 3.6411547660827637\n",
      "Itérations 39: loss-train 13.670655250549316   lostt-test 3.5911364555358887\n",
      "Itérations 40: loss-train 13.467758178710938   lostt-test 3.5425570011138916\n",
      "Itérations 41: loss-train 13.271849632263184   lostt-test 3.495349884033203\n",
      "Itérations 42: loss-train 13.082571029663086   lostt-test 3.4494521617889404\n",
      "Itérations 43: loss-train 12.8996000289917   lostt-test 3.4048068523406982\n",
      "Itérations 44: loss-train 12.722631454467773   lostt-test 3.3613598346710205\n",
      "Itérations 45: loss-train 12.551382064819336   lostt-test 3.319061279296875\n",
      "Itérations 46: loss-train 12.385587692260742   lostt-test 3.277865409851074\n",
      "Itérations 47: loss-train 12.225005149841309   lostt-test 3.237729072570801\n",
      "Itérations 48: loss-train 12.06939697265625   lostt-test 3.198611259460449\n",
      "Itérations 49: loss-train 11.918550491333008   lostt-test 3.160475254058838\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand((out_dim, nb_features), requires_grad=True)\n",
    "B = torch.rand(out_dim, requires_grad=True)\n",
    "NB_EPOCHS = 50\n",
    "batch_size = 128\n",
    "epsilon = 1e-3\n",
    "writter = SummaryWriter()\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    for i in range(len(X_train)//batch_size):\n",
    "        x_batch = X_train[i : i+batch_size]\n",
    "        y_batch = Y_train[i : i+batch_size]\n",
    "        y_hat_batch = forward(x_batch, W, B)\n",
    "        loss_batch = mse(y_hat_batch, y_batch)\n",
    "        loss_batch.backward()\n",
    "        with torch.no_grad():\n",
    "            W -= epsilon*W.grad\n",
    "            B -= epsilon*B.grad\n",
    "            W.grad = None\n",
    "            B.grad = None\n",
    "\n",
    "    loss_train = mse(forward(X_train, W, B), Y_train)\n",
    "    loss_val = mse(forward(X_val, W, B), Y_val)\n",
    "\n",
    "    writter.add_scalar('Loss/train', loss_train, epoch)\n",
    "    writter.add_scalar('Loss/val', loss_val, epoch)\n",
    "    # Sortie directe\n",
    "    print(f\"Itérations {epoch}: loss-train {loss_train}   lostt-test {loss_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons sur tensorboard et les résultats que la descente de gradient stochastique converge plus rapide que mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=input_dim, out_features=hidden_dim),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=hidden_dim, out_features=out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations 0: loss-train 0.5396705865859985   lostt-test 0.5422253608703613\n",
      "Itérations 1: loss-train 0.5325914025306702   lostt-test 0.5351741909980774\n",
      "Itérations 2: loss-train 0.5256171822547913   lostt-test 0.5282277464866638\n",
      "Itérations 3: loss-train 0.5187463164329529   lostt-test 0.5213843584060669\n",
      "Itérations 4: loss-train 0.5119768977165222   lostt-test 0.5146423578262329\n",
      "Itérations 5: loss-train 0.5053076148033142   lostt-test 0.5080002546310425\n",
      "Itérations 6: loss-train 0.4987366795539856   lostt-test 0.5014562606811523\n",
      "Itérations 7: loss-train 0.49226248264312744   lostt-test 0.4950089454650879\n",
      "Itérations 8: loss-train 0.4858837425708771   lostt-test 0.48865681886672974\n",
      "Itérations 9: loss-train 0.47959867119789124   lostt-test 0.48239827156066895\n",
      "Itérations 10: loss-train 0.4734059274196625   lostt-test 0.47623181343078613\n",
      "Itérations 11: loss-train 0.4673040807247162   lostt-test 0.47015610337257385\n",
      "Itérations 12: loss-train 0.4612915813922882   lostt-test 0.4641696810722351\n",
      "Itérations 13: loss-train 0.45536720752716064   lostt-test 0.4582710564136505\n",
      "Itérations 14: loss-train 0.4495295286178589   lostt-test 0.4524590075016022\n",
      "Itérations 15: loss-train 0.44377702474594116   lostt-test 0.4467321038246155\n",
      "Itérations 16: loss-train 0.4381086230278015   lostt-test 0.441089004278183\n",
      "Itérations 17: loss-train 0.4325227737426758   lostt-test 0.43552839756011963\n",
      "Itérations 18: loss-train 0.42701834440231323   lostt-test 0.4300491213798523\n",
      "Itérations 19: loss-train 0.42159411311149597   lostt-test 0.42464980483055115\n",
      "Itérations 20: loss-train 0.41624873876571655   lostt-test 0.4193292558193207\n",
      "Itérations 21: loss-train 0.41098105907440186   lostt-test 0.41408631205558777\n",
      "Itérations 22: loss-train 0.40578988194465637   lostt-test 0.40891966223716736\n",
      "Itérations 23: loss-train 0.400674045085907   lostt-test 0.4038282632827759\n",
      "Itérations 24: loss-train 0.39563238620758057   lostt-test 0.39881086349487305\n",
      "Itérations 25: loss-train 0.39066368341445923   lostt-test 0.39386633038520813\n",
      "Itérations 26: loss-train 0.3857669532299042   lostt-test 0.38899362087249756\n",
      "Itérations 27: loss-train 0.3809410333633423   lostt-test 0.384191632270813\n",
      "Itérations 28: loss-train 0.3761848509311676   lostt-test 0.37945935130119324\n",
      "Itérations 29: loss-train 0.37149742245674133   lostt-test 0.37479549646377563\n",
      "Itérations 30: loss-train 0.36687761545181274   lostt-test 0.37019920349121094\n",
      "Itérations 31: loss-train 0.36232441663742065   lostt-test 0.36566948890686035\n",
      "Itérations 32: loss-train 0.35783684253692627   lostt-test 0.3612053096294403\n",
      "Itérations 33: loss-train 0.3534139096736908   lostt-test 0.3568055033683777\n",
      "Itérations 34: loss-train 0.34905463457107544   lostt-test 0.35246938467025757\n",
      "Itérations 35: loss-train 0.3447580635547638   lostt-test 0.34819576144218445\n",
      "Itérations 36: loss-train 0.34052324295043945   lostt-test 0.343983918428421\n",
      "Itérations 37: loss-train 0.3363493084907532   lostt-test 0.33983269333839417\n",
      "Itérations 38: loss-train 0.3322352468967438   lostt-test 0.3357413709163666\n",
      "Itérations 39: loss-train 0.3281802833080292   lostt-test 0.3317089378833771\n",
      "Itérations 40: loss-train 0.32418346405029297   lostt-test 0.3277345895767212\n",
      "Itérations 41: loss-train 0.3202439546585083   lostt-test 0.3238174617290497\n",
      "Itérations 42: loss-train 0.31636086106300354   lostt-test 0.31995657086372375\n",
      "Itérations 43: loss-train 0.3125334084033966   lostt-test 0.3161512315273285\n",
      "Itérations 44: loss-train 0.3087606728076935   lostt-test 0.31240057945251465\n",
      "Itérations 45: loss-train 0.30504196882247925   lostt-test 0.30870380997657776\n",
      "Itérations 46: loss-train 0.3013764023780823   lostt-test 0.30506008863449097\n",
      "Itérations 47: loss-train 0.29776322841644287   lostt-test 0.3014686405658722\n",
      "Itérations 48: loss-train 0.29420164227485657   lostt-test 0.29792872071266174\n",
      "Itérations 49: loss-train 0.29069095849990845   lostt-test 0.29443955421447754\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCHS = 50\n",
    "\n",
    "model = monNN(input_dim=nb_features, out_dim=out_dim, hidden_dim=nb_features)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "writter = SummaryWriter()\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    yhat = model.forward(X_train)\n",
    "    loss = loss_fn(yhat, Y_train)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    writter.add_scalar('Loss/train', loss, epoch)\n",
    "    # evaluation\n",
    "    yhat_eval = model.forward(X_val)\n",
    "    loss_eval = loss_fn(yhat_eval,Y_val)\n",
    "    writter.add_scalar('Loss/test', loss_eval, epoch)\n",
    "    # Sortie directe\n",
    "    print(f\"Itérations {epoch}: loss-train {loss}   lostt-test {loss_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
