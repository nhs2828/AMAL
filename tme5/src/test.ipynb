{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "import string\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## Token de padding (BLANK)\n",
    "PAD_IX = 0\n",
    "## Token de fin de séquence\n",
    "EOS_IX = 1\n",
    "\n",
    "LETTRES = string.ascii_letters + string.punctuation + string.digits + ' '\n",
    "id2lettre = dict(zip(range(2, len(LETTRES)+2), LETTRES))\n",
    "id2lettre[PAD_IX] = '<PAD>' ##NULL CHARACTER\n",
    "id2lettre[EOS_IX] = '<EOS>'\n",
    "lettre2id = dict(zip(id2lettre.values(),id2lettre.keys()))\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\" enlève les accents et les caractères spéciaux\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if  c in LETTRES)\n",
    "\n",
    "def string2code(s):\n",
    "    \"\"\"prend une séquence de lettres et renvoie la séquence d'entiers correspondantes\"\"\"\n",
    "    return torch.tensor([lettre2id[c] for c in normalize(s)])\n",
    "\n",
    "def code2string(t):\n",
    "    \"\"\" prend une séquence d'entiers et renvoie la séquence de lettres correspondantes \"\"\"\n",
    "    if type(t) !=list:\n",
    "        t = t.tolist()\n",
    "    return ''.join(id2lettre[i] for i in t)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, *, maxsent=None, maxlen=None):\n",
    "        \"\"\"  Dataset pour les tweets de Trump\n",
    "            * fname : nom du fichier\n",
    "            * maxsent : nombre maximum de phrases.\n",
    "            * maxlen : longueur maximale des phrases.\n",
    "        \"\"\"\n",
    "        maxlen = maxlen or sys.maxsize\n",
    "        self.phrases = [re.sub(' +',' ',p[:maxlen]).strip() +\".\" for p in text.split(\".\") if len(re.sub(' +',' ',p[:maxlen]).strip())>0]\n",
    "        if maxsent is not None:\n",
    "            self.phrases=self.phrases[:maxsent]\n",
    "        self.maxlen = max([len(p) for p in self.phrases])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phrases)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return string2code(self.phrases[i])\n",
    "\n",
    "def pad_collate_fn(samples: List[List[int]]):\n",
    "    #  TODO:  Renvoie un batch à partir d'une liste de listes d'indexes (de phrases) qu'il faut padder.\n",
    "    maxlen = max([len(p) for p in samples]) + 1 # +1 pour eos\n",
    "    data = torch.empty(size=(len(samples), maxlen))\n",
    "    for i, phrase in enumerate(samples):\n",
    "        tmp = F.pad(phrase, pad=(0,1), mode='constant', value=lettre2id['<EOS>'])\n",
    "        data[i] = F.pad(tmp, pad=(0,maxlen-len(tmp)), mode='constant', value=lettre2id['<PAD>'])\n",
    "    return data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaîne à code :  C'est. Un. Test.\n",
      "Shape ok\n",
      "encodage OK\n",
      "Token EOS ok\n",
      "Token BLANK ok\n",
      "Chaîne décodée :  C'est. Un. Test.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test = \"C'est. Un. Test.\"\n",
    "    ds = TextDataset(test)\n",
    "    loader = DataLoader(ds, collate_fn=pad_collate_fn, batch_size=3)\n",
    "    data = next(iter(loader))\n",
    "    print(\"Chaîne à code : \", test)\n",
    "    # Longueur maximum\n",
    "    assert data.shape == (7, 3)\n",
    "    print(\"Shape ok\")\n",
    "    # e dans les deux cas\n",
    "    assert data[2, 0] == data[1, 2]\n",
    "    print(\"encodage OK\")\n",
    "    # Token EOS présent\n",
    "    assert data[5,2] == EOS_IX\n",
    "    print(\"Token EOS ok\")\n",
    "    # BLANK présent\n",
    "    assert (data[4:,1]==0).sum() == data.shape[0]-4\n",
    "    print(\"Token BLANK ok\")\n",
    "    # les chaînes sont identiques\n",
    "    s_decode = \" \".join([code2string(s).replace(id2lettre[PAD_IX],\"\").replace(id2lettre[EOS_IX],\"\") for s in data.t()])\n",
    "    print(\"Chaîne décodée : \", s_decode)\n",
    "    assert test == s_decode\n",
    "    # \" \".join([code2string(s).replace(id2lettre[PAD_IX],\"\").replace(id2lettre[EOS_IX],\"\") for s in data.t()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedCrossEntropy(output: torch.Tensor, target: torch.LongTensor, padcar: int):\n",
    "    \"\"\"\n",
    "    :param output: Tenseur length x batch x output_dim, pas encore softmax ??\n",
    "    :param target: Tenseur length x batch\n",
    "    :param padcar: index du caractere de padding\n",
    "    \"\"\"\n",
    "    sm_output = torch.log(torch.softmax(output, dim=-1)) # log softmax\n",
    "    masque_target = torch.where(target == padcar, 0., 1.)\n",
    "    index_target = target.unsqueeze(-1) # leng, batch, 1\n",
    "    loss = -torch.gather(sm_output, dim=0, index=index_target).squeeze(-1)*masque_target\n",
    "    return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        # define les vars\n",
    "        self.input_size = input_size # ou embedding dimension\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        # Long term memory\n",
    "        # Short term memory\n",
    "        # forget gate\n",
    "        self.f = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_size+self.hidden_size, out_features=self.hidden_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        # input gate\n",
    "        self.i = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_size+self.hidden_size, out_features=self.hidden_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.g = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_size+self.hidden_size, out_features=self.hidden_size),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        # output gate\n",
    "        self.o = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_size+self.hidden_size, out_features=self.hidden_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        # decode\n",
    "        self.decode = torch.nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "\n",
    "    def one_step(self, x, c, h):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x: input au moment t\n",
    "            c: long term memory au moment t-1\n",
    "            h: short term memory au moment t-1\n",
    "        \"\"\"\n",
    "        # forget gate\n",
    "        f_t = self.f(torch.cat([x, h], dim=-1))\n",
    "        c_t = torch.mul(c, f_t)\n",
    "        # input gate\n",
    "        i_t = self.i(torch.cat([x, h], dim=-1))\n",
    "        g_t = self.g(torch.cat([x, h], dim=-1))\n",
    "        c_t = torch.add(c_t, torch.mul(i_t, g_t))\n",
    "        # output gate\n",
    "        o_t = self.o(torch.cat([x, h], dim=-1))\n",
    "        h_t = torch.mul(o_t, torch.tanh(c_t))\n",
    "\n",
    "        return c_t, h_t\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x: input, tensor 2-D (Length, Batch, embedding_dim)\n",
    "        \"\"\"\n",
    "        len_seq, batch_size = x.shape[0], x.shape[1]\n",
    "        c, h = self.init_long_short_term(batch_size)\n",
    "        h_full = torch.empty(size=(len_seq, batch_size, self.hidden_size))\n",
    "        for i in range(len_seq):\n",
    "            c, h = self.one_step(x[i], c, h)\n",
    "            h_full[i] = h\n",
    "        return h_full\n",
    "    \n",
    "    def decode(self, h, training = True):\n",
    "        if training:\n",
    "            return self.decode(h) # raw logits, softmax dans loss fonction correspondant CELoss in torch\n",
    "        return torch.softmax(self.decode(h), dim=-1)\n",
    "\n",
    "    def init_long_short_term(self, batch_size):\n",
    "        return torch.zeros(size=(batch_size, self.hidden_size)), torch.zeros(size=(batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = torch.nn.Embedding(num_embeddings=10, embedding_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3361,  0.0434, -0.8069, -0.2445,  0.2521],\n",
       "        [ 0.6464,  1.1616,  1.0572,  1.3739, -0.5292],\n",
       "        [ 0.2144,  1.5302, -0.3469, -0.1844,  1.0415],\n",
       "        [-0.3753,  0.8793,  1.8172,  1.0014,  1.0258]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = em(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7616])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(torch.tensor([-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
